{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/right-stack/AI-Capstone-Project/blob/main/Resnet_18_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkTMmvyXiFv0"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN0rJM7viFv5"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGNFx-KtiFv6"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRS39mcKiFv7"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olNuWVxkiFv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKOTcZlziFv8"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je0JkveJiFv8"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZphBhzNiFv9",
        "outputId": "492c9e04-e299-4cef-c896-2da15e0684fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-25 15:00:37--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  31.6MB/s    in 75s     \n",
            "\n",
            "2022-03-25 15:01:52 (33.2 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q0GLFEdCiFv_"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqQ1zBI_iFv_",
        "outputId": "3031dfc4-948b-4bab-8cd7-0f8db568506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-25 15:03:40--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  38.3MB/s    in 57s     \n",
            "\n",
            "2022-03-25 15:04:38 (35.3 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6sHva2JiFwA"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCtT9QGZiFwA",
        "outputId": "1eb2bf53-6286-4e6f-e4a8-37defc03dfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPislyVDiFwA"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBGihdg0iFwB"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ZKlKciiFwB",
        "outputId": "54c07e0c-e3a2-4aa3-f50f-c5735120cc40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8c76c83470>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RJ7hEdemiFwC"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8gqfIlwiFwC"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVerBUnxiFwE"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl4NHwQniFwE"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdbFA0ZJiFwE",
        "outputId": "073d1d07-49de-4c2d-d952-a7052b69fd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content/\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSFF2CPUiFwF"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7TJ3lzHiFwF",
        "outputId": "6807905b-1898-45e9-bdeb-c1cbbf66cedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(transform=None, train=True)\n",
        "validation_dataset = Dataset(transform=None, train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wyXePWRiFwG"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWhRkXXiFwG"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztFDYQ4qiFwG"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "96ac47629d9745e88d3a679115855c7d",
            "f4e4f29635524ead9bf59b492895f670",
            "9d03e146c92845d9b78fab10cb807c3d",
            "cd7e1e31b8e34a5ebf5e57793333ba66",
            "503ceaf36b7640f8967dda9eb4057bf3",
            "78c8e5c9baaa467895a6a505f9e55de4",
            "7db9743d1d4840158e7b5e381fd72057",
            "66ed6a2a50564e47946eeb8e1ebe14c9",
            "4fb71bb018ef445091b196bb54e9e52b",
            "306173fefbef42809ab2340b41686824",
            "925e4872c34641c5add9c5f322dbac64"
          ]
        },
        "id": "JV_-4YjbiFwG",
        "outputId": "b809c4e1-9b4f-44c3-e474-cdbb1728e2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96ac47629d9745e88d3a679115855c7d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "# Type your code here\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81DrM75iFwG"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6tryx7kHiFwG"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "# Type your code here\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPTCDjMRiFwH"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29q1zfWiFwH"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IVW7oA-hiFwH"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIEWJeYyiFwH"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB2T-gMwiFwH",
        "outputId": "c366e37d-32fe-48c2-dd4e-f4c2c8b8a411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYctw8zxiFwH"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jauAYz3YiFwI"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Kua4S5iFwI"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "42K__QvxiFwI"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "# Type your code here\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbw_PtziiFwI"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5g5FXDwIiFwI"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6syaWe0iFwI"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AqoCOwHSiFwJ"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mANiwa8SiFwJ",
        "outputId": "b482f842-70a9-4999-ac5f-37285f65212a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJcQ8JsiFwJ"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxbSPG9iFwJ"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BEa-HTAliFwJ"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "# n_epochs = 20\n",
        "\n",
        "# model_input_size = 3 * 224 * 224 \n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "        # calculate loss \n",
        "        loss = criterion(z, y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct += (yhat == y_test).sum().item()\n",
        "    \n",
        "    accuracy=correct/N_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7u0daZyiFwJ"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UUvXYCJiFwK",
        "outputId": "6d9200ef-def7-4818-e0ba-9e667825af3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9946"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "QPldw1YYiFwK",
        "outputId": "2a8b855d-bf27-4d8f-e104-bbf52b8857ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpE7PNHbiFwK"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okoGa1KGiFwK"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2Gy2ltE8iFwK"
      },
      "outputs": [],
      "source": [
        "incorrect=0\n",
        "for x_test, y_test in validation_loader:\n",
        "  # set model to eval \n",
        "  model.eval()\n",
        "  #make a prediction \n",
        "  z = model(x_test)\n",
        "  #find max \n",
        "  _, yhat = torch.max(z.data, 1)\n",
        "  torch.eq(yhat, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUGwubk4CE0f",
        "outputId": "a12e3792-fff4-4627-95c8-7721bfd3496d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FSrl9Ho23N5",
        "outputId": "4ba44670-1c20-4df2-8805-99c9b6afad93"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "        1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.eq(yhat, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SwNrEddCPjH",
        "outputId": "47c75bff-e2cb-4bc5-bf0f-ae8dde198a44"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn8eVjiAEHOi",
        "outputId": "001df476-a9b0-42af-a965-714c6e18972f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeAXQR5biFwK"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMOn4Fq_iFwL"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgB0R2yWiFwL"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Resnet-18-Pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96ac47629d9745e88d3a679115855c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4e4f29635524ead9bf59b492895f670",
              "IPY_MODEL_9d03e146c92845d9b78fab10cb807c3d",
              "IPY_MODEL_cd7e1e31b8e34a5ebf5e57793333ba66"
            ],
            "layout": "IPY_MODEL_503ceaf36b7640f8967dda9eb4057bf3"
          }
        },
        "f4e4f29635524ead9bf59b492895f670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78c8e5c9baaa467895a6a505f9e55de4",
            "placeholder": "​",
            "style": "IPY_MODEL_7db9743d1d4840158e7b5e381fd72057",
            "value": "100%"
          }
        },
        "9d03e146c92845d9b78fab10cb807c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ed6a2a50564e47946eeb8e1ebe14c9",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fb71bb018ef445091b196bb54e9e52b",
            "value": 46830571
          }
        },
        "cd7e1e31b8e34a5ebf5e57793333ba66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_306173fefbef42809ab2340b41686824",
            "placeholder": "​",
            "style": "IPY_MODEL_925e4872c34641c5add9c5f322dbac64",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 97.5MB/s]"
          }
        },
        "503ceaf36b7640f8967dda9eb4057bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c8e5c9baaa467895a6a505f9e55de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db9743d1d4840158e7b5e381fd72057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66ed6a2a50564e47946eeb8e1ebe14c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fb71bb018ef445091b196bb54e9e52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "306173fefbef42809ab2340b41686824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925e4872c34641c5add9c5f322dbac64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}